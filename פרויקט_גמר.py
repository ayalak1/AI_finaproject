# -*- coding: utf-8 -*-
"""פרויקט גמר

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/135Xc4-S-rpDHyoGv3DJ4GI9JqTLiFenq
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import xgboost as xgb
from sklearn.metrics import confusion_matrix, classification_report
from collections import Counter
import matplotlib.pyplot as plt
from xgboost import plot_importance
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_curve, roc_auc_score

df = pd.read_csv('/content/drive/MyDrive/פרויקט גמר/sp500_daily.csv')
df

df.isna().any(axis=1).sum()

df = df.dropna()

df = df.drop(index=0).reset_index(drop=True)

df['Open_tomorrow'] = df['Open'].shift(-1)

df['Open_gt_Close'] = (df['Open_tomorrow'] > df['Close']).astype(int)

df['Close_gt_Open'] = (df['Close'] > df['Close'].shift(1)).astype(int)

df = df.dropna(subset=['Open_tomorrow'])

df

df['Close_tomorrow'] = df['Close'].shift(-1)

k_days=13
test_size = 0.2
features = ["Volume","Open","High",]
target = 'Close_gt_Open'

X = pd.concat([df[features].shift(i) for i in reversed(range(k_days))], axis=1)
X.columns = [f"{feat}_t-{i}" for i in reversed(range(k_days)) for feat in features]
y = df[target]

X = X.iloc[k_days-1:].reset_index(drop=True)
y = y.iloc[k_days-1:].reset_index(drop=True)

X_np = X.to_numpy().astype(float)
y_np = y.to_numpy().astype(int)

print(X_np[2,:])

split_idx = int(len(X_np) * (1 - test_size))
X_train, X_test = X_np[:split_idx], X_np[split_idx:]
y_train, y_test = y_np[:split_idx], y_np[split_idx:]

print(X_train.shape,y_train.shape)

print(X_test.shape,y_test.shape)

# #Symmetry dont use
# counter = Counter(y_train)
# n_class_0 = counter[0]
# n_class_1 = counter[1]
# max_class = max(n_class_0, n_class_1)

# # יוצרים Train סימטרי ע"י שכפול בלוקים קטנים
# X_train_0 = X_train[y_train == 0]
# X_train_1 = X_train[y_train == 1]

# # שכפול
# X_train_0_up = np.tile(X_train_0, (int(np.ceil(max_class / len(X_train_0))), 1))[:max_class]
# X_train_1_up = np.tile(X_train_1, (int(np.ceil(max_class / len(X_train_1))), 1))[:max_class]

# y_train_0_up = np.zeros(max_class, dtype=int)
# y_train_1_up = np.ones(max_class, dtype=int)

# # מחברים חזרה
# X_train = np.vstack([X_train_0_up, X_train_1_up])
# y_train = np.concatenate([y_train_0_up, y_train_1_up])

unique, counts = np.unique(y_train, return_counts=True)
print(dict(zip(unique, counts)))

counter = Counter(y_train)
weight = counter[0] / counter[1]
model = xgb.XGBClassifier(
    scale_pos_weight=weight  ,
    n_estimators=344,
    max_depth=7  ,
    learning_rate=0.02,
    # use_label_encoder=False,
     eval_metric='logloss'

)
 # Adjust y_train to match X_train's shape
# X_train = X_train[:,:-1]

model.fit(X_train, y_train)

X_test = X_test[:,:-1]

y_pred=model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

model.get_booster().feature_names = features

k_days = 13 # מספר הימים אחורה שאת בודקת

feature_names = []

# יצירת השמות עבור המערך
for col in features:
    feature_names.append(f"{col}_current") # הערך של היום
    for i in range(1, k_days ):
        feature_names.append(f"{col}_lag_{i}") # הערכים של ימי העבר
print(feature_names)

# 1. עדכון שמות הפיצ'רים בתוך ה-Booster של המודל
# חשוב: ודאי שהסדר ב-feature_names תואם בדיוק לסדר העמודות ב-X_train

# Reconstruct the feature names based on how X was originally built and then sliced
# original_features_for_X = ["Volume", "Open", "High"]
# full_original_X_columns = []
# for i in reversed(range(k_days)): # i goes from k_days-1 down to 0
#     for feat in original_features_for_X:
#         full_original_X_columns.append(f"{feat}_t-{i}")

# X_train was sliced twice by [:,:-1], so remove the last two feature names
corrected_feature_names = feature_names
print (len(corrected_feature_names))
model.get_booster().feature_names = corrected_feature_names

# 2. הגדרת גודל הגרף (כדי שיהיה מקום לשמות ארוכים בצד)
plt.rcParams["figure.figsize"] = (10, 8)

# 3. יצירת גרף הבארים (Feature Importance)
# הערה: העברנו את ה-importance_type ל-'gain' כי הוא משמעותי יותר בחיזוי מניות
plot_importance(model,
                max_num_features=10, # נציג את ה-10 הכי חזקים מתוך הרשימה
                importance_type='gain',
                grid=False,
                show_values=False, # כדי שהגרף יהיה נקי
                title='S&P 500 Feature Importance (Top 10)',
                xlabel='Gain (Predictive Power)',
                ylabel='Features')

plt.tight_layout() # מבטיח שהשמות לא ייחתכו בשוליים
plt.show()

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()
# k_days=13
# test_size = 0.2
# features = ["Volume","Open","High",]
# target = 'Close_gt_Open'

print(X_test.shape,y_test.shape)

print(X_train.shape,y_train.shape)

# Ensure X_test has the same number of features as the trained model (37 features)
# X_test = X_test[:,:-1]

# 1. קבלת ההסתברויות מהמודל (הסתברות למחלקה 1 - עלייה)
y_probs = model.predict_proba(X_test)[:, 1]

# 2. חישוב נתוני ה-ROC
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc_value = roc_auc_score(y_test, y_probs)

# 3. ציור הגרף
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_value:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # קו האלכסון (רנדומלי)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity / Recall)')
plt.title('ROC Curve - S&P 500 Prediction')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()



# k_days=20
# test_size = 0.3
# features = ["Open","Volume"]
# target = 'Close_gt_Open'

# counter = Counter(y_train)
# weight = counter[0] / counter[1]
# model = xgb.XGBClassifier(
#     scale_pos_weight=weight,
#     n_estimators=350,
#     max_depth=6,
#     learning_rate=0.02,
#     # use_label_encoder=False,
#      eval_metric='logloss'

# )
# model.fit(X_train, y_train)
# Accuracy =0.5222

# k_days=9
# test_size = 0.3
# features = ["Volume","Open","High"]
# target = 'Close_gt_Open'

# counter = Counter(y_train)
# weight = counter[0] / counter[1]
# model = xgb.XGBClassifier(
#     scale_pos_weight=weight,
#     n_estimators=350,
#     max_depth=6,
#     learning_rate=0.02,
#     # use_label_encoder=False,
#      eval_metric='logloss'

# )
# model.fit(X_train, y_train)
# Accuracy=0.5254

k_days=13
test_size = 0.3
features = ["Volume","Open","High","Open_gt_Close"]
target = 'Close_gt_Open'

counter = Counter(y_train)
weight = counter[0] / counter[1]
model = xgb.XGBClassifier(
    scale_pos_weight=weight  ,#344 6 0.02,
    n_estimators=344,
    max_depth=6  ,
    learning_rate=0.02,
    # use_label_encoder=False,
     eval_metric='logloss'

)
 # Adjust y_train to match X_train's shape
X_train = X_train[:,:-1]
# y_train = y_train[:-1]
model.fit(X_train, y_train)